{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Dataset for classification","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nfrom sklearn.datasets import make_classification\n# create dataframe\nX, y = make_classification(n_samples=1000,\n                           n_features=20,\n                           n_informative=15,\n                           n_redundant=5,\n                           random_state=1)\n# print the data classes info\nprint(f'''Main dataframe:\nNumber of samples: {X.shape[0]}\nNumber of features: {X.shape[1]}\nSamples by class:''')\ncounter = Counter(y)\nfor k, v in counter.items():\n    per = v / len(y) * 100\n    print('Class=%d, Count=%d, Percentage=%.1f%%' % (k, v, per))\n    \n# Main dataframe:\n# Number of samples: 1000\n# Number of features: 20\n# Samples by class:\n# Class=0, Count=501, Percentage=50.1%\n# Class=1, Count=499, Percentage=49.9%","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-06T09:15:34.119076Z","iopub.execute_input":"2022-06-06T09:15:34.119465Z","iopub.status.idle":"2022-06-06T09:15:34.131136Z","shell.execute_reply.started":"2022-06-06T09:15:34.119438Z","shell.execute_reply":"2022-06-06T09:15:34.130094Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Cross-validation","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    # define the evaluation procedure\n    cv = RepeatedStratifiedKFold(random_state=1)\n    # evaluate the model and return results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores","metadata":{"execution":{"iopub.status.busy":"2022-06-06T09:15:34.141156Z","iopub.execute_input":"2022-06-06T09:15:34.141605Z","iopub.status.idle":"2022-06-06T09:15:34.148412Z","shell.execute_reply.started":"2022-06-06T09:15:34.141571Z","shell.execute_reply":"2022-06-06T09:15:34.147616Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"List of classifiers","metadata":{}},{"cell_type":"code","source":"# dummy\nfrom sklearn.dummy import DummyClassifier\n# ensemble\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom xgboost import XGBClassifier\n# gaussian_process\nfrom sklearn.gaussian_process import GaussianProcessClassifier\n# linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import SGDClassifier\n# naive_bayes\nfrom sklearn.naive_bayes import BernoulliNB\n# neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import NearestCentroid\n# neural_network\nfrom sklearn.neural_network import MLPClassifier\n# tree\nfrom sklearn.tree import DecisionTreeClassifier\n# support vectors\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\n# discriminant analysis\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n# define the models to evaluate\nmodels = dict()\nmodels['dummy'] = DummyClassifier(random_state=1)\nmodels['adaboost'] = AdaBoostClassifier(random_state=1)\nmodels['bagging'] = BaggingClassifier(random_state=1)\nmodels['extrs'] = ExtraTreesClassifier(random_state=1)\nmodels['grdboost'] = GradientBoostingClassifier(random_state=1)\nmodels['randfor'] = RandomForestClassifier(random_state=1)\nmodels['histgrdboost'] = HistGradientBoostingClassifier(random_state=1)\nmodels['xgboost'] = XGBClassifier(random_state=1)\nmodels['gausproc'] = GaussianProcessClassifier(random_state=1)\nmodels['logreg'] = LogisticRegression(random_state=1)\nmodels['pasagr'] = PassiveAggressiveClassifier(random_state=1)\nmodels['prcptr'] = Perceptron(random_state=1)\nmodels['ridge'] = RidgeClassifier(random_state=1)\nmodels['sgd'] = SGDClassifier(random_state=1)\nmodels['bernnb'] = BernoulliNB()\nmodels['kneighb'] = KNeighborsClassifier()\nmodels['nearcent'] = NearestCentroid()\nmodels['mlp'] = MLPClassifier(max_iter=2000, random_state=1)\nmodels['dtree'] = DecisionTreeClassifier(random_state=1)\nmodels['svc'] = SVC(random_state=1)\nmodels['lsvc'] = LinearSVC(max_iter=5000, random_state=1)\nmodels['ldiscranal'] = LinearDiscriminantAnalysis()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T09:15:34.173750Z","iopub.execute_input":"2022-06-06T09:15:34.174297Z","iopub.status.idle":"2022-06-06T09:15:34.192231Z","shell.execute_reply.started":"2022-06-06T09:15:34.174263Z","shell.execute_reply":"2022-06-06T09:15:34.191294Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Data Transformers","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PowerTransformer\n# define data transformation techniques\ndef get_pipelines(model):\n    pipelines = list()\n    # normalize\n    p = Pipeline([('s', MinMaxScaler()), ('m', model)])\n    pipelines.append(('norm', p))\n    # standardize\n    p = Pipeline([('s', StandardScaler()), ('m', model)])\n    pipelines.append(('std', p))\n    # quantile\n    p = Pipeline([('s', QuantileTransformer(n_quantiles=800, random_state=1)), ('m', model)])\n    pipelines.append(('quan', p))\n    # power\n    p = Pipeline([('s', PowerTransformer()), ('m', model)])\n    pipelines.append(('pow', p))\n    return pipelines","metadata":{"execution":{"iopub.status.busy":"2022-06-06T09:15:34.197698Z","iopub.execute_input":"2022-06-06T09:15:34.198093Z","iopub.status.idle":"2022-06-06T09:15:34.214197Z","shell.execute_reply.started":"2022-06-06T09:15:34.198062Z","shell.execute_reply":"2022-06-06T09:15:34.213433Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Grid search for the best models and best data preparation techniques","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\nfrom numpy import mean, std\n# evaluate the models and store results\ndef pipeline_grid_searcher():\n    # define results lists\n    results, m_names, p_names = list(), list(), list()\n    for m_name, model in models.items():\n        print('\\nmodel:', type(model).__name__)\n        # get cross-validation scores\n        scores = evaluate_model(model, X, y)\n        # store the results\n        results.append(scores)\n        # store the model names\n        m_names.append(m_name)\n        # get current time\n        current_time = datetime.now().strftime(\"%H:%M:%S\")\n        # show the results\n        print('%s %s %.3f %.3f' % (current_time, m_name, mean(scores), std(scores)))\n        # evaluate the pipelines\n        for p_name, pipeline in get_pipelines(model):\n            # get cross-validation scores\n            scores = evaluate_model(pipeline, X, y)\n            # store the results\n            results.append(scores)\n            # store preprocessing names\n            p_names.append(p_name)\n            current_time = datetime.now().strftime(\"%H:%M:%S\")\n            print('%s %s %s %.3f %.3f' % (current_time, p_name, m_name, mean(scores), std(scores)))\n    return results, m_names, p_names","metadata":{"execution":{"iopub.status.busy":"2022-06-06T09:15:34.217509Z","iopub.execute_input":"2022-06-06T09:15:34.218030Z","iopub.status.idle":"2022-06-06T09:15:34.233321Z","shell.execute_reply.started":"2022-06-06T09:15:34.217999Z","shell.execute_reply":"2022-06-06T09:15:34.232145Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Call grid searcher","metadata":{}},{"cell_type":"code","source":"# call the function\nresults, m_names, p_names = pipeline_grid_searcher()\n\n# model: DummyClassifier\n# 08:07:03 dummy 0.501 0.002\n# 08:07:03 norm dummy 0.501 0.002\n# 08:07:03 std dummy 0.501 0.002\n# 08:07:04 quan dummy 0.501 0.002\n# 08:07:06 pow dummy 0.501 0.002\n#\n# model: AdaBoostClassifier\n# 08:07:10 adaboost 0.857 0.022\n# 08:07:13 norm adaboost 0.857 0.022\n# 08:07:17 std adaboost 0.857 0.022\n# 08:07:22 quan adaboost 0.857 0.022\n# 08:07:27 pow adaboost 0.857 0.022\n#\n# model: BaggingClassifier\n# 08:07:29 bagging 0.882 0.021\n# 08:07:31 norm bagging 0.882 0.021\n# 08:07:33 std bagging 0.882 0.021\n# 08:07:35 quan bagging 0.881 0.020\n# 08:07:38 pow bagging 0.882 0.021\n#\n# model: ExtraTreesClassifier\n# 08:07:42 extrs 0.935 0.018\n# 08:07:46 norm extrs 0.935 0.018\n# 08:07:50 std extrs 0.935 0.018\n# 08:07:55 quan extrs 0.931 0.018\n# 08:08:01 pow extrs 0.937 0.018\n#\n# model: GradientBoostingClassifier\n# 08:08:11 grdboost 0.915 0.021\n# 08:08:21 norm grdboost 0.915 0.021\n# 08:08:31 std grdboost 0.915 0.021\n# 08:08:41 quan grdboost 0.915 0.021\n# 08:08:53 pow grdboost 0.915 0.021\n#\n# model: RandomForestClassifier\n# 08:09:00 randfor 0.918 0.018\n# 08:09:06 norm randfor 0.918 0.018\n# 08:09:13 std randfor 0.918 0.018\n# 08:09:21 quan randfor 0.918 0.017\n# 08:09:28 pow randfor 0.917 0.018\n#\n# model: HistGradientBoostingClassifier\n# 08:09:38 histgrdboost 0.926 0.017\n# 08:09:47 norm histgrdboost 0.926 0.017\n# 08:09:57 std histgrdboost 0.926 0.017\n# 08:10:07 quan histgrdboost 0.926 0.017\n# 08:10:18 pow histgrdboost 0.926 0.017\n#\n# model: XGBClassifier\n# 08:10:30 xgboost 0.921 0.019\n# 08:10:42 norm xgboost 0.921 0.019\n# 08:10:54 std xgboost 0.921 0.019\n# 08:11:06 quan xgboost 0.921 0.019\n# 08:11:19 pow xgboost 0.921 0.019\n#\n# model: GaussianProcessClassifier\n# 08:11:25 gausproc 0.909 0.017\n# 08:11:33 norm gausproc 0.894 0.020\n# 08:11:38 std gausproc 0.958 0.012\n# 08:11:46 quan gausproc 0.933 0.017\n# 08:11:53 pow gausproc 0.959 0.012\n#\n# model: LogisticRegression\n# 08:11:53 logreg 0.867 0.019\n# 08:11:54 norm logreg 0.867 0.019\n# 08:11:54 std logreg 0.867 0.019\n# 08:11:55 quan logreg 0.869 0.019\n# 08:11:57 pow logreg 0.866 0.019\n#\n# model: PassiveAggressiveClassifier\n# 08:11:57 pasagr 0.806 0.039\n# 08:11:57 norm pasagr 0.804 0.077\n# 08:11:57 std pasagr 0.810 0.033\n# 08:11:58 quan pasagr 0.819 0.059\n# 08:11:59 pow pasagr 0.808 0.033\n#\n# model: Perceptron\n# 08:12:00 prcptr 0.820 0.031\n# 08:12:00 norm prcptr 0.806 0.068\n# 08:12:00 std prcptr 0.820 0.038\n# 08:12:01 quan prcptr 0.824 0.047\n# 08:12:02 pow prcptr 0.820 0.035\n#\n# model: RidgeClassifier\n# 08:12:02 ridge 0.866 0.018\n# 08:12:02 norm ridge 0.866 0.018\n# 08:12:03 std ridge 0.867 0.018\n# 08:12:03 quan ridge 0.873 0.021\n# 08:12:05 pow ridge 0.869 0.019\n#\n# model: SGDClassifier\n# 08:12:05 sgd 0.814 0.037\n# 08:12:05 norm sgd 0.842 0.036\n# 08:12:05 std sgd 0.838 0.028\n# 08:12:06 quan sgd 0.851 0.026\n# 08:12:08 pow sgd 0.836 0.029\n#\n# model: BernoulliNB\n# 08:12:08 bernnb 0.793 0.023\n# 08:12:08 norm bernnb 0.507 0.010\n# 08:12:08 std bernnb 0.779 0.028\n# 08:12:09 quan bernnb 0.507 0.010\n# 08:12:10 pow bernnb 0.778 0.026\n#\n# model: KNeighborsClassifier\n# 08:12:11 kneighb 0.925 0.016\n# 08:12:11 norm kneighb 0.953 0.010\n# 08:12:11 std kneighb 0.954 0.012\n# 08:12:12 quan kneighb 0.952 0.012\n# 08:12:14 pow kneighb 0.956 0.011\n#\n# model: NearestCentroid\n# 08:12:14 nearcent 0.714 0.030\n# 08:12:14 norm nearcent 0.818 0.025\n# 08:12:14 std nearcent 0.820 0.025\n# 08:12:15 quan nearcent 0.817 0.023\n# 08:12:17 pow nearcent 0.821 0.025\n#\n# model: MLPClassifier\n# 08:12:38 mlp 0.950 0.013\n# 08:14:00 norm mlp 0.947 0.013\n# 08:14:26 std mlp 0.954 0.012\n# 08:15:33 quan mlp 0.948 0.014\n# 08:16:01 pow mlp 0.954 0.013\n#\n# model: DecisionTreeClassifier\n# 08:16:01 dtree 0.815 0.029\n# 08:16:02 norm dtree 0.815 0.029\n# 08:16:02 std dtree 0.815 0.029\n# 08:16:03 quan dtree 0.815 0.028\n# 08:16:05 pow dtree 0.815 0.029\n#\n# model: SVC\n# 08:16:05 svc 0.950 0.017\n# 08:16:06 norm svc 0.963 0.013\n# 08:16:07 std svc 0.966 0.012\n# 08:16:08 quan svc 0.959 0.014\n# 08:16:10 pow svc 0.966 0.012\n#\n# model: LinearSVC\n# 08:16:13 lsvc 0.867 0.020\n# 08:16:14 norm lsvc 0.868 0.017\n# 08:16:15 std lsvc 0.867 0.020\n# 08:16:16 quan lsvc 0.871 0.020\n# 08:16:19 pow lsvc 0.867 0.019\n#\n# model: LinearDiscriminantAnalysis\n# 08:16:19 ldiscranal 0.866 0.018\n# 08:16:19 norm ldiscranal 0.866 0.018\n# 08:16:19 std ldiscranal 0.866 0.018\n# 08:16:20 quan ldiscranal 0.872 0.020\n# 08:16:22 pow ldiscranal 0.869 0.019","metadata":{"execution":{"iopub.status.busy":"2022-06-06T09:15:34.254147Z","iopub.execute_input":"2022-06-06T09:15:34.254884Z","iopub.status.idle":"2022-06-06T09:24:35.585322Z","shell.execute_reply.started":"2022-06-06T09:15:34.254828Z","shell.execute_reply":"2022-06-06T09:24:35.584333Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Sorted result","metadata":{}},{"cell_type":"code","source":"from numpy import repeat\nfrom pandas import DataFrame\n# create list with preprocessing names\nprep = [x for y in (['row'] + p_names[i:i + 4] * (i < len(p_names) - 2)\n                    for i in range(0, len(p_names), 4)) for x in y]\n# sort the results\ndf_results = DataFrame(list(zip(repeat(m_names, 5),\n                                prep,\n                                [round(mean(x), 3) for x in results],\n                                [round(std(x), 3) for x in results])),\n                       columns=['model', 'prep', 'mean', 'std']).sort_values(by=['mean'], ascending=False).head(n=20)\nprint('\\nSummary of the best models:')\nprint(df_results)\n\n# Summary of the best models:\n#        model  prep   mean    std\n# 99       svc   pow  0.966  0.012\n# 97       svc   std  0.966  0.012\n# 96       svc  norm  0.963  0.013\n# 44  gausproc   pow  0.959  0.012\n# 98       svc  quan  0.959  0.014\n# 42  gausproc   std  0.958  0.012\n# 79   kneighb   pow  0.956  0.011\n# 77   kneighb   std  0.954  0.012\n# 89       mlp   pow  0.954  0.013\n# 87       mlp   std  0.954  0.012\n# 76   kneighb  norm  0.953  0.010\n# 78   kneighb  quan  0.952  0.012\n# 95       svc   row  0.950  0.017\n# 85       mlp   row  0.950  0.013\n# 88       mlp  quan  0.948  0.014\n# 86       mlp  norm  0.947  0.013\n# 19     extrs   pow  0.937  0.018\n# 16     extrs  norm  0.935  0.018\n# 17     extrs   std  0.935  0.018\n# 15     extrs   row  0.935  0.018","metadata":{"execution":{"iopub.status.busy":"2022-06-06T09:24:35.586895Z","iopub.execute_input":"2022-06-06T09:24:35.587279Z","iopub.status.idle":"2022-06-06T09:24:35.612916Z","shell.execute_reply.started":"2022-06-06T09:24:35.587249Z","shell.execute_reply":"2022-06-06T09:24:35.612271Z"},"trusted":true},"execution_count":15,"outputs":[]}]}